<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
  <title>Bing Fan — Academic Homepage</title>
  <link rel="stylesheet" href="styles.css">
  <link rel="stylesheet" href="https://unpkg.com/leaflet@1.9.4/dist/leaflet.css" integrity="sha256-p4NxAoJBhIIN+hmNHrzRCf9tD/miZyoHS5obTRR9BMY=" crossorigin="">
  <meta name="description" content="Personal homepage of Bing Fan — research, publications, and contact.">
</head>
<body>
  <header class="site-header">
    <div class="container">
      <div class="header-left">
        <img src="images/person.jpg" alt="Bing Fan" class="avatar">
        <!-- Add your UNT logo file as images/unt-logo.png to display it here -->
        <img src="images/unt-logo.png" alt="University of North Texas" class="university-logo" onerror="this.style.display='none'">
      </div>
      <div class="title-block">
        <h1>Bing Fan</h1>
        <p class="subtitle">Ph.D. Student, Computer Science and Engineering, University of North Texas</p>
        <p class="meta">Email: <a href="mailto:bingfan@my.unt.edu">bingfan@my.unt.edu</a></p>
        <div class="social-links">
          <a href="mailto:bingfan@my.unt.edu" target="_blank" rel="noopener noreferrer">Email</a>
          <span class="separator">/</span>
          <a href="https://www.linkedin.com/in/bing-fan-a77881300/" target="_blank" rel="noopener noreferrer">LinkedIn</a>
          <span class="separator">/</span>
          <a href="https://scholar.google.com/citations?user=pcZrxOsAAAAJ&hl" target="_blank" rel="noopener noreferrer">Google Scholar</a>
          <span class="separator">/</span>
          <a href="BingFan_CV.pdf" target="_blank" rel="noopener noreferrer">CV</a>
        </div>
      </div>
    </div>
  </header>

  <nav class="nav">
    <div class="container">
      <a href="#about">About</a>
      <a href="#publications">Publications</a>
      <a href="#research">Research</a>
      <a href="#contact">Contact</a>
    </div>
  </nav>

  <main class="container">

    <section id="about" class="card visible">
      <h2>About</h2>
      <p>I am a Ph.D. student in Computer Science and Engineering at the University of North Texas (UNT), advised by Prof. Heng Fan. My research focuses on video understanding, multimedia forensics, and computer vision.</p>
      <p>I previously received an M.S. in Cyberspace Security from Nanchang University, China, advised by Prof. Feng Ding, and a B.S. in Software Engineering from Nanchang University, China.</p>
    </section>

    <section id="research" class="card">
      <h2>Research</h2>
      <p>My research interests include:</p>
      <ul>
        <li><strong>Video understanding</strong> — egocentric visual query localization, ego–exo correspondence, and omni-object video grounding.</li>
        <li><strong>Multimedia forensics</strong> — detection and analysis of AI-generated and manipulated media.</li>
        <li><strong>Computer vision</strong> — robust visual understanding across complex real-world scenes.</li>
      </ul>
    </section>

    <section id="publications" class="card">
      <h2>Publications</h2>
      <p style="margin-bottom: 0;">For a complete list, see my <a href="https://scholar.google.com/citations?user=pcZrxOsAAAAJ&hl" target="_blank" rel="noopener noreferrer">Google Scholar</a>.</p>
      
      <h3 class="publication-category">Video Understanding</h3>
      <div class="publications-list">
        <div class="publication-item">
          <div class="publication-figure">
            <img src="images/papers/paper8.png" alt="Paper 8 Figure" class="paper-thumbnail">
          </div>
          <div class="publication-content">
            <h3 class="publication-title">OmniSTVG: Toward Spatio-Temporal Omni-Object Video Grounding</h3>
            <p class="publication-authors">J. Yao, X. Deng, X. Gu, M. Dai, <strong>B. Fan</strong>, Z. Zhang, Y. Huang, H. Fan, L. Zhang</p>
            <p class="publication-venue">International Conference on Learning Representations (ICLR), 2026</p>
            <p class="publication-links">
              <a href="https://arxiv.org/abs/2503.10500" class="paper-link">paper</a>
              <a href="https://github.com/JellyYao3000/OmniSTVG" class="code-link">code</a>
            </p>
          </div>
        </div>

        <div class="publication-item">
          <div class="publication-figure">
            <img src="images/papers/paper1.png" alt="Paper 1 Figure" class="paper-thumbnail">
          </div>
          <div class="publication-content">
            <h3 class="publication-title">Robust Ego-Exo Correspondence with Long-Term Memory</h3>
            <p class="publication-authors">Y. Hu*, <strong>B. Fan*</strong>, X. Gu, H. Ren, D. Liu, H. Fan, L. Zhang</p>
            <p class="publication-venue">Neural Information Processing Systems (NeurIPS), 2025</p>
            <p class="publication-links">
              <a href="https://arxiv.org/abs/2510.11417" class="paper-link">paper</a>
              <a href="https://github.com/juneyeeHu/LM-EEC" class="code-link">code</a>
            </p>
          </div>
        </div>

        <div class="publication-item">
          <div class="publication-figure">
            <img src="images/papers/paper4.png" alt="Paper 4 Figure" class="paper-thumbnail">
          </div>
          <div class="publication-content">
            <h3 class="publication-title">PRVQL: Progressive Knowledge-Guided Refinement for Robust Egocentric Visual Query Localization</h3>
            <p class="publication-authors"><strong>B. Fan</strong>, Y. Feng, Y. Tian, J. C. Liang, Y. Lin, Y. Huang, H. Fan</p>
            <p class="publication-venue">International Conference on Computer Vision (ICCV), 2025</p>
            <p class="publication-links">
              <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Fan_PRVQL_Progressive_Knowledge-guided_Refinement_for_Robust_Egocentric_Visual_Query_Localization_ICCV_2025_paper.pdf" class="paper-link">paper</a>
              <a href="https://github.com/fb-reps/PRVQL" class="code-link">code</a>
            </p>
          </div>
        </div>
      </div>

      <h3 class="publication-category">Multimedia Forensics</h3>
      <div class="publications-list">
        <div class="publication-item">
          <div class="publication-figure">
            <img src="images/papers/paper2.png" alt="Paper 2 Figure" class="paper-thumbnail">
          </div>
          <div class="publication-content">
            <h3 class="publication-title">Breaking Latent Prior Bias in Detectors for Generalizable AIGC Image Detection</h3>
            <p class="publication-authors">Y. Zhou, X. He, K. Lin, <strong>B. Fan</strong>, F. Ding, B. Li</p>
            <p class="publication-venue">Neural Information Processing Systems (NeurIPS), 2025</p>
            <p class="publication-links">
              <a href="https://arxiv.org/pdf/2506.00874" class="paper-link">paper</a>
            </p>
          </div>
        </div>

        <div class="publication-item">
          <div class="publication-figure">
            <img src="images/papers/paper3.png" alt="Paper 3 Figure" class="paper-thumbnail">
          </div>
          <div class="publication-content">
            <h3 class="publication-title">VLForgery Face Triad: Detection, Localization and Attribution via Multimodal Large Language Models</h3>
            <p class="publication-authors">X. He, Y. Zhou, <strong>B. Fan</strong>, B. Li, G. Zhu, F. Ding</p>
            <p class="publication-venue">Neural Information Processing Systems (NeurIPS), 2025</p>
            <p class="publication-links">
              <a href="https://arxiv.org/pdf/2503.06142" class="paper-link">paper</a>
            </p>
          </div>
        </div>

        <div class="publication-item">
          <div class="publication-figure">
            <img src="images/papers/paper5.png" alt="Paper 5 Figure" class="paper-thumbnail">
          </div>
          <div class="publication-content">
            <h3 class="publication-title">Generating Higher-Quality Anti-Forensics DeepFakes with Adversarial Sharpening Mask</h3>
            <p class="publication-authors"><strong>B. Fan</strong>, F. Ding, G. Zhu, J. Huang, S. Kwong, P. Atrey, S. Lyu</p>
            <p class="publication-venue">ACM Transactions on Multimedia Computing, Communications and Applications (TOMM), 2025</p>
            <p class="publication-links">
              <a href="https://dl.acm.org/doi/full/10.1145/3729233" class="paper-link">paper</a>
            </p>
          </div>
        </div>

        <div class="publication-item">
          <div class="publication-figure">
            <img src="images/papers/paper6.png" alt="Paper 6 Figure" class="paper-thumbnail">
          </div>
          <div class="publication-content">
            <h3 class="publication-title">Synthesizing Black-Box Anti-Forensics DeepFakes with High Visual Quality</h3>
            <p class="publication-authors"><strong>B. Fan</strong>, S. Hu, F. Ding</p>
            <p class="publication-venue">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024</p>
            <p class="publication-links">
              <a href="https://arxiv.org/abs/2312.10713" class="paper-link">paper</a>
              <a href="https://github.com/fb-reps/Synthesizing-Black-box-Anti-forensics-DeepFakes-with-High-Visual-Quality" class="code-link">code</a>
            </p>
          </div>
        </div>

        <div class="publication-item">
          <div class="publication-figure">
            <img src="images/papers/paper7.png" alt="Paper 7 Figure" class="paper-thumbnail">
          </div>
          <div class="publication-content">
            <h3 class="publication-title">Securing Facial Bioinformation by Eliminating Adversarial Perturbations</h3>
            <p class="publication-authors">F. Ding, <strong>B. Fan</strong>, Z. Shen, K. Yu, G. Srivastava, K. Dev, S. Wan</p>
            <p class="publication-venue">IEEE Transactions on Industrial Informatics, 19(5): 6682–6691, 2022</p>
            <p class="publication-links">
              <a href="https://ieeexplore.ieee.org/document/9866824" class="paper-link">paper</a>
            </p>
          </div>
        </div>
      </div>
    </section>

    <section id="contact" class="card">
      <h2>Contact</h2>
      <p><strong>Email:</strong> <a href="mailto:bingfan@my.unt.edu">bingfan@my.unt.edu</a></p>
      <p><strong>Institution:</strong> University of North Texas (UNT)</p>
      <p>
        <strong>Connect:</strong><br>
        <a href="https://www.linkedin.com/in/bing-fan-a77881300/" target="_blank" rel="noopener noreferrer">LinkedIn</a> | 
        <a href="https://scholar.google.com/citations?user=pcZrxOsAAAAJ&hl" target="_blank" rel="noopener noreferrer">Google Scholar</a>
      </p>
      <p><strong>CV:</strong> <a href="BingFan_CV.pdf" target="_blank">Download PDF</a></p>
    </section>

    <section id="visitor-map" class="card">
      <h2>Visitor Map</h2>
      <p>See where visitors to this page are located (approximate).</p>
      <div class="visitor-stats">
        <div class="visitor-count">
          <span class="visitor-number" id="visitor-number">…</span>
          <span class="visitor-label">Total visitors</span>
        </div>
      </div>
      <div id="world-map" class="world-map"></div>
      <p class="map-note">The number above is total page views. The map shows the world view; for a map with visitor locations by country, you can add a free tracker (e.g. <a href="https://clustrmaps.com" target="_blank" rel="noopener">ClustrMaps</a> or <a href="https://www.revolvermaps.com" target="_blank" rel="noopener">RevolverMaps</a>).</p>
    </section>

  </main>

  <footer class="site-footer">
    <div class="container">
      <p>© Bing Fan. Built with a lightweight template.</p>
      <p>Source: <a href="#">GitHub</a></p>
    </div>
  </footer>

  <script src="https://unpkg.com/leaflet@1.9.4/dist/leaflet.js" integrity="sha256-20nQCchB9co0qIjJZRGuk2/Z9VM+kNiyxNV1lvTlZBo=" crossorigin=""></script>
  <script>
    (function() {
      var observer = new IntersectionObserver(function(entries) {
        entries.forEach(function(entry) {
          if (entry.isIntersecting) {
            entry.target.classList.add('visible');
          }
        });
      }, { rootMargin: '0px 0px -40px 0px', threshold: 0.1 });
      document.querySelectorAll('.card, .publication-category').forEach(function(el) {
        observer.observe(el);
      });
    })();

    (function hashScroll() {
      var hash = window.location.hash;
      if (!hash) return;
      var el = document.getElementById(hash.slice(1));
      if (el) {
        el.classList.add('visible');
        setTimeout(function() {
          el.scrollIntoView({ behavior: 'smooth', block: 'start' });
        }, 100);
      }
    })();

    (function initMap() {
      var mapEl = document.getElementById('world-map');
      if (!mapEl) return;
      var map = L.map('world-map', { scrollWheelZoom: false }).setView([20, 0], 2);
      L.tileLayer('https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', {
        attribution: '&copy; <a href="https://www.openstreetmap.org/copyright">OpenStreetMap</a>'
      }).addTo(map);
      mapEl._leaflet = map;
    })();

    (function visitorCount() {
      var el = document.getElementById('visitor-number');
      if (!el) return;
      var key = 'bingfan-homepage/visits';
      fetch('https://api.countapi.xyz/hit/' + key)
        .then(function(r) { return r.json(); })
        .then(function(d) { el.textContent = d.value != null ? d.value.toLocaleString() : '—'; })
        .catch(function() { el.textContent = '—'; });
    })();
  </script>
</body>
</html>
